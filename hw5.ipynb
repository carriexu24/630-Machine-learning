{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Sentiment analysis with word embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import string\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data from files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_text(path_dir, prefix, flag):\n",
    "    file_path = path_dir+prefix+'/'+flag+'/'\n",
    "    result = []\n",
    "    for fi in os.listdir(file_path):\n",
    "        with open(file_path+fi) as f:   \n",
    "            result.append(f.read())\n",
    "    return result\n",
    "\n",
    "def process_file(path_dir='aclImdb/', prefix='train'):\n",
    "    # check output file\n",
    "    # if exists, delete it\n",
    "    output_file = '%s%s.csv' % (path_dir, prefix)\n",
    "    if os.path.exists(output_file): os.remove(output_file)\n",
    "    \n",
    "    # process pos and neg files\n",
    "    pos_result = read_text(path_dir, prefix, 'pos')\n",
    "    neg_result = read_text(path_dir, prefix, 'neg')\n",
    "    \n",
    "    # write output file \n",
    "    f = open(output_file, 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('text', 'flag'))\n",
    "    writer.writerows((t,1) for t in pos_result)\n",
    "    writer.writerows((t,0) for t in neg_result)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_file(path_dir='aclImdb/', prefix='train')\n",
    "process_file(path_dir='aclImdb/', prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('aclImdb/train.csv')\n",
    "test = pd.read_csv('aclImdb/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use the libary `spacy` to tokenize your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# borrowed from fast.ai (https://github.com/fastai/fastai/blob/master/fastai/nlp.py)\n",
    "\n",
    "re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "def sub_br(x): return re_br.sub(\"\\n\", x)\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(sub_br(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Download embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read the 300 dimensional Glove embeddings into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(filename):\n",
    "    \"\"\"\n",
    "    Read all lines from the indicated file and return a dictionary\n",
    "    mapping word:vector where vectors are of numpy `array` type.\n",
    "    GloVe file lines are of the form:\n",
    "    the 0.418 0.24968 -0.41242 0.1217 ...\n",
    "    So split each line on spaces into a list; the first element is the word\n",
    "    and the remaining elements represent factor components. The length of the vector\n",
    "    should not matter; read vectors of any length.\n",
    "    \"\"\"\n",
    "    dict = {}\n",
    "\n",
    "    f = open(filename)\n",
    "    for line in f.readlines():\n",
    "        split_line = line.split()\n",
    "        for i in range(1, len(split_line)):\n",
    "            split_line[i] = float(split_line[i])\n",
    "\n",
    "        dict[split_line[0]] = np.array(split_line[1:])\n",
    "    f.close()\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gloves = load_glove('/Users/chuanxu/data/glove/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create average feature embedding for each sentence. You may want to ignore stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modified from https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def get_non_stopwords(sentence):\n",
    "    \"\"\"Returns a list of non-stopwords\"\"\"\n",
    "    return np.array([gloves[x] for x in spacy_tok(str(sentence).lower()) if (x not in stops) and (x in gloves.keys())])\n",
    "\n",
    "def get_average_feature_embedding(sentence):\n",
    "    return np.mean(get_non_stopwords(sentence), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 1s, sys: 444 ms, total: 4min 2s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['text'] = data['text'].apply(get_average_feature_embedding)\n",
    "test['text'] = test['text'].apply(get_average_feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_pickle('train')\n",
    "test.to_pickle('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fit an XGBoost classifier to this data. Report test and training errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "data = pd.read_pickle('train')\n",
    "test = pd.read_pickle('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scores(bst, d, y, data_set):\n",
    "    y_hat_prob = bst.predict(d)\n",
    "    y_hat = [int(x > 0.5) for x in y_hat_prob]\n",
    "    return '%s accuracy: %f\\n%s logloss: %f' % (data_set, accuracy_score(y, y_hat), data_set, log_loss(y, y_hat_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = np.array([x for x in data['text'].values])\n",
    "y_data = data['flag'].values\n",
    "\n",
    "X_test = np.array([x for x in test['text'].values])\n",
    "y_test = test['flag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.680421\tvalid-logloss:0.682339\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.422993\tvalid-logloss:0.487192\n",
      "[100]\ttrain-logloss:0.337639\tvalid-logloss:0.438117\n",
      "[150]\ttrain-logloss:0.290643\tvalid-logloss:0.4156\n",
      "[200]\ttrain-logloss:0.258256\tvalid-logloss:0.402363\n",
      "[250]\ttrain-logloss:0.235819\tvalid-logloss:0.39412\n",
      "[300]\ttrain-logloss:0.217094\tvalid-logloss:0.388836\n",
      "[350]\ttrain-logloss:0.201216\tvalid-logloss:0.385207\n",
      "[399]\ttrain-logloss:0.188329\tvalid-logloss:0.382689\n"
     ]
    }
   ],
   "source": [
    "xgb_pars = {\"min_child_weight\": 50, \"eta\": 0.05, \"max_depth\": 8,\n",
    "            \"subsample\": 0.8, \"silent\" : 1, \"nthread\": 4,\n",
    "            \"eval_metric\": \"logloss\", \"objective\": \"binary:logistic\"}\n",
    "\n",
    "d_data = xgb.DMatrix(X_data, label=y_data)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_data, 'train'), (d_test, 'valid')]\n",
    "\n",
    "bst = xgb.train(xgb_pars, d_data, 400, watchlist, early_stopping_rounds=50, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.958720\n",
      "train logloss: 0.188329\n",
      "test accuracy: 0.826120\n",
      "test logloss: 0.382689\n"
     ]
    }
   ],
   "source": [
    "print(scores(bst, d_data, y_data, 'train'))\n",
    "print(scores(bst, d_test, y_test, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare previous results to fitting XGBoost to a one-hot encoding representation of the data with bag of words. Report test and training errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('aclImdb/train.csv')\n",
    "test = pd.read_csv('aclImdb/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scores(bst, d, y, data_set):\n",
    "    y_hat_prob = bst.predict(d)\n",
    "    y_hat = [int(x > 0.5) for x in y_hat_prob]\n",
    "    return '%s accuracy: %f\\n%s logloss: %f' % (data_set, accuracy_score(y, y_hat), data_set, log_loss(y, y_hat_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "count_vect = CountVectorizer(stop_words = stops, lowercase = True, binary = True).fit(data['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data = count_vect.transform(data['text'].tolist())\n",
    "y_data = data['flag'].values\n",
    "\n",
    "X_test = count_vect.transform(test['text'].tolist())\n",
    "y_test = test['flag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.681675\tvalid-logloss:0.681531\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.489659\tvalid-logloss:0.494371\n",
      "[100]\ttrain-logloss:0.428559\tvalid-logloss:0.439625\n",
      "[150]\ttrain-logloss:0.393221\tvalid-logloss:0.408737\n",
      "[200]\ttrain-logloss:0.368887\tvalid-logloss:0.388962\n",
      "[250]\ttrain-logloss:0.351054\tvalid-logloss:0.374743\n",
      "[300]\ttrain-logloss:0.337585\tvalid-logloss:0.364973\n",
      "[350]\ttrain-logloss:0.326808\tvalid-logloss:0.35713\n",
      "[399]\ttrain-logloss:0.317598\tvalid-logloss:0.351301\n"
     ]
    }
   ],
   "source": [
    "xgb_pars = {\"min_child_weight\": 50, \"eta\": 0.05, \"max_depth\": 8,\n",
    "            \"subsample\": 0.8, \"silent\" : 1, \"nthread\": 4,\n",
    "            \"eval_metric\": \"logloss\", \"objective\": \"binary:logistic\"}\n",
    "\n",
    "d_data = xgb.DMatrix(X_data, label=y_data)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_data, 'train'), (d_test, 'valid')]\n",
    "\n",
    "bst = xgb.train(xgb_pars, d_data, 400, watchlist, early_stopping_rounds=50, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.873440\n",
      "train logloss: 0.317598\n",
      "test accuracy: 0.849600\n",
      "test logloss: 0.351301\n"
     ]
    }
   ],
   "source": [
    "print(scores(bst, d_data, y_data, 'train'))\n",
    "print(scores(bst, d_test, y_test, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
